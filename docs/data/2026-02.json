[
  {
    "id": "2602.11626v1",
    "title": "ArGEnT: Arbitrary Geometry-encoded Transformer for Operator Learning",
    "authors": "Wenqian Chen, Yucheng Fu, Michael Penwarden, Pratanu Roy, Panos Stinis",
    "abstract": "Learning solution operators for systems with complex, varying geometries and parametric physical settings is a central challenge in scientific machine learning. In many-query regimes such as design optimization, control and inverse problems, surrogate modeling must generalize across geometries while allowing flexible evaluation at arbitrary spatial locations. In this work, we propose Arbitrary Geometry-encoded Transformer (ArGEnT), a geometry-aware attention-based architecture for operator learning on arbitrary domains. ArGEnT employs Transformer attention mechanisms to encode geometric information directly from point-cloud representations with three variants-self-attention, cross-attention, and hybrid-attention-that incorporates different strategies for incorporating geometric features. By integrating ArGEnT into DeepONet as the trunk network, we develop a surrogate modeling framework capable of learning operator mappings that depend on both geometric and non-geometric inputs without the need to explicitly parametrize geometry as a branch network input. Evaluation on benchmark problems spanning fluid dynamics, solid mechanics and electrochemical systems, we demonstrate significantly improved prediction accuracy and generalization performance compared with the standard DeepONet and other existing geometry-aware saurrogates. In particular, the cross-attention transformer variant enables accurate geometry-conditioned predictions with reduced reliance on signed distance functions. By combining flexible geometry encoding with operator-learning capabilities, ArGEnT provides a scalable surrogate modeling framework for optimization, uncertainty quantification, and data-driven modeling of complex physical systems.",
    "published": "2026-02-12",
    "arxiv_url": "http://arxiv.org/abs/2602.11626v1",
    "pdf_url": "https://arxiv.org/pdf/2602.11626v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "physics.comp-ph",
      "physics.flu-dyn"
    ],
    "conference": "",
    "code_link": "",
    "tags": [
      "流体力学",
      "智能流体力学",
      "CFD与机器学习交叉",
      "机器学习"
    ],
    "official_keywords": [],
    "custom_keywords": [
      "rans",
      "surrogate model",
      "fluid dynamics",
      "data-driven",
      "machine learning",
      "data-driven modeling",
      "transformer",
      "les",
      "rom"
    ],
    "keywords": [
      "rans",
      "surrogate model",
      "data-driven",
      "fluid dynamics",
      "machine learning",
      "data-driven modeling",
      "transformer",
      "les",
      "rom"
    ],
    "citation_count": null,
    "impact_factor": null
  },
  {
    "id": "2602.02832v1",
    "title": "Koopman Autoencoders with Continuous-Time Latent Dynamics for Fluid Dynamics Forecasting",
    "authors": "Rares Grozavescu, Pengyu Zhang, Etienne Meunier, Mark Girolami",
    "abstract": "Data-driven surrogate models have emerged as powerful tools for accelerating the simulation of turbulent flows. However, classical approaches which perform autoregressive rollouts often trade off between strong short-term accuracy and long-horizon stability. Koopman autoencoders, inspired by Koopman operator theory, provide a physics-based alternative by mapping nonlinear dynamics into a latent space where linear evolution is conducted. In practice, most existing formulations operate in a discrete-time setting, limiting temporal flexibility. In this work, we introduce a continuous-time Koopman framework that models latent evolution through numerical integration schemes. By allowing variable timesteps at inference, the method demonstrates robustness to temporal resolution and generalizes beyond training regimes. In addition, the learned dynamics closely adhere to the analytical matrix exponential solution, enabling efficient long-horizon forecasting. We evaluate the approach on classical CFD benchmarks and report accuracy, stability, and extrapolation properties.",
    "published": "2026-02-02",
    "arxiv_url": "http://arxiv.org/abs/2602.02832v1",
    "pdf_url": "https://arxiv.org/pdf/2602.02832v1",
    "categories": [
      "cs.LG",
      "physics.flu-dyn"
    ],
    "conference": "",
    "code_link": "",
    "tags": [
      "流体力学",
      "智能流体力学",
      "CFD与机器学习交叉"
    ],
    "official_keywords": [],
    "custom_keywords": [
      "cfd",
      "surrogate model",
      "fluid dynamics",
      "data-driven"
    ],
    "keywords": [
      "cfd",
      "surrogate model",
      "fluid dynamics",
      "data-driven"
    ],
    "citation_count": null,
    "impact_factor": null
  },
  {
    "id": "2602.01737v1",
    "title": "Physics-Informed Chebyshev Polynomial Neural Operator for Parametric Partial Differential Equations",
    "authors": "Biao Chen, Jing Wang, Hairun Xie, Qineng Wang, Shuai Zhang, Yifan Xia, Jifa Zhang",
    "abstract": "Neural operators have emerged as powerful deep learning frameworks for approximating solution operators of parameterized partial differential equations (PDE). However, current methods predominantly rely on multilayer perceptrons (MLPs) for mapping inputs to solutions, which impairs training robustness in physics-informed settings due to inherent spectral biases and fixed activation functions. To overcome the architectural limitations, we introduce the Physics-Informed Chebyshev Polynomial Neural Operator (CPNO), a novel mesh-free framework that leverages a basis transformation to replace unstable monomial expansions with the numerically stable Chebyshev spectral basis. By integrating parameter dependent modulation mechanism to main net, CPNO constructs PDE solutions in a near-optimal functional space, decoupling the model from MLP-specific constraints and enhancing multi-scale representation. Theoretical analysis demonstrates the Chebyshev basis's near-minimax uniform approximation properties and superior conditioning, with Lebesgue constants growing logarithmically with degree, thereby mitigating spectral bias and ensuring stable gradient flow during optimization. Numerical experiments on benchmark parameterized PDEs show that CPNO achieves superior accuracy, faster convergence, and enhanced robustness to hyperparameters. The experiment of transonic airfoil flow has demonstrated the capability of CPNO in characterizing complex geometric problems.",
    "published": "2026-02-02",
    "arxiv_url": "http://arxiv.org/abs/2602.01737v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01737v1",
    "categories": [
      "physics.flu-dyn",
      "cs.LG"
    ],
    "conference": "",
    "code_link": "",
    "tags": [
      "智能流体力学",
      "机器学习"
    ],
    "official_keywords": [],
    "custom_keywords": [
      "rom",
      "rans",
      "deep learning"
    ],
    "keywords": [
      "rans",
      "rom",
      "deep learning"
    ],
    "citation_count": null,
    "impact_factor": null
  },
  {
    "id": "2602.01379v1",
    "title": "WAKESET: A Large-Scale, High-Reynolds Number Flow Dataset for Machine Learning of Turbulent Wake Dynamics",
    "authors": "Zachary Cooper-Baldock, Paulo E. Santos, Russell S. A. Brinkworth, Karl Sammut",
    "abstract": "Machine learning (ML) offers transformative potential for computational fluid dynamics (CFD), promising to accelerate simulations, improve turbulence modelling, and enable real-time flow prediction and control-capabilities that could fundamentally change how engineers approach fluid dynamics problems. However, the exploration of ML in fluid dynamics is critically hampered by the scarcity of large, diverse, and high-fidelity datasets suitable for training robust models. This limitation is particularly acute for highly turbulent flows, which dominate practical engineering applications yet remain computationally prohibitive to simulate at scale. High-Reynolds number turbulent datasets are essential for ML models to learn the complex, multi-scale physics characteristic of real-world flows, enabling generalisation beyond the simplified, low-Reynolds number regimes often represented in existing datasets. This paper introduces WAKESET, a novel, large-scale CFD dataset of highly turbulent flows, designed to address this critical gap. The dataset captures the complex hydrodynamic interactions during the underwater recovery of an autonomous underwater vehicle by a larger extra-large uncrewed underwater vehicle. It comprises 1,091 high-fidelity Reynolds-Averaged Navier-Stokes simulations, augmented to 4,364 instances, covering a wide operational envelope of speeds (up to Reynolds numbers of 1.09 x 10^8) and turning angles. This work details the motivation for this new dataset by reviewing existing resources, outlines the hydrodynamic modelling and validation underpinning its creation, and describes its structure. The dataset's focus on a practical engineering problem, its scale, and its high turbulence characteristics make it a valuable resource for developing and benchmarking ML models for flow field prediction, surrogate modelling, and autonomous navigation in complex underwater environments.",
    "published": "2026-02-01",
    "arxiv_url": "http://arxiv.org/abs/2602.01379v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01379v1",
    "categories": [
      "physics.flu-dyn",
      "cs.LG"
    ],
    "conference": "",
    "code_link": "",
    "tags": [
      "流体力学",
      "智能流体力学",
      "CFD与机器学习交叉",
      "机器学习"
    ],
    "official_keywords": [],
    "custom_keywords": [
      "rans",
      "surrogate model",
      "fluid dynamics",
      "machine learning",
      "computational fluid dynamics",
      "pinn",
      "les",
      "cfd",
      "rom",
      "turbulence"
    ],
    "keywords": [
      "rans",
      "surrogate model",
      "fluid dynamics",
      "machine learning",
      "computational fluid dynamics",
      "pinn",
      "les",
      "cfd",
      "rom",
      "turbulence"
    ],
    "citation_count": null,
    "impact_factor": null
  }
]